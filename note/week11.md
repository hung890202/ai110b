## 神經網路
* 簡介
在電腦領域，神經網路是指一種模擬神經系統所設計出來的程式，用來模擬人類視覺、聽覺等等智慧行為的原理，企圖讓電腦可以具有人類智慧的一種方法。 

下圖是生物神經細胞的結構圖，這個圖看來頗為複雜，如果電腦程式真的要模擬這麼複雜的結構，那程式應該也會非常複雜才對。

![Pic](https://github.com/hung890202/ai110b/blob/master/note/%E5%9C%96%E7%89%87/神經網路.jpg)

但我們不需要設計出高基氏體、細胞膜等人類的細胞胞，我們的目的就是設計出類似行為的程式,人類可以透過抽象化將細胞結構簡化成下圖(單一神經元,單一神經網路)。

![Pic](https://github.com/brian891005/ai109b/blob/main/Note/%E5%9C%96%E7%89%87/電腦神經網路.jpg)

## 從微分到梯度下降法
單變數微分 <--------------->微分單變數

* diff.py
```
PS C:\Users\rick2\ai\07-neural\02-gradient\01-diff> python diff.py
diff(f,2)= 4.000999999999699

```

* e.py
```
n= 100.0 e(n)= 2.7048138294215285
n= 200.0 e(n)= 2.711517122929317 
n= 300.0 e(n)= 2.7137651579427837
n= 400.0 e(n)= 2.7148917443812293
n= 500.0 e(n)= 2.715568520651728 
n= 600.0 e(n)= 2.7160200488806514
n= 700.0 e(n)= 2.7163427377295566
n= 800.0 e(n)= 2.716584846682471
n= 900.0 e(n)= 2.716773208380411
n= 1000.0 e(n)= 2.7169239322355936
.
.
.
n= 9000.0 e(n)= 2.7181308281830128
n= 9100.0 e(n)= 2.718132487359168
n= 9200.0 e(n)= 2.718134110467929
n= 9300.0 e(n)= 2.718135698675662
n= 9400.0 e(n)= 2.718137253097062
n= 9500.0 e(n)= 2.7181387748001744
n= 9700.0 e(n)= 2.718141724076723
n= 9800.0 e(n)= 2.718143153583405
n= 9900.0 e(n)= 2.718144554210053
n= 10000.0 e(n)= 2.7181459268249255

```

## 梯度下降法
深度學習 (Deep Learning) 是人工智慧領域當紅的技術，說穿了其實就是原本的《神經網路》(Neural Network) ，不過由於加上了一些新的模型 (像是捲積神經網路 CNN, 循環神經網路 RNN 與生成對抗網路 GAN)，還有在神經網路的層數上加深很多，從以往的 3-4 層，提升到了十幾層，甚至上百層，於是我們給這些新一代的《神經網路》技術一個統稱，那就是《深度學習》。 

雖然《深度學習》的神經網路層數變多了，《網路模型》也多了一些，但是背後的學習算法和運作原理並沒有多大改變，仍然是以《梯度下降》(Gradient Descendent) 和《反傳遞算法》(Back Propagation) 為主。

* 梯度就是斜率最大的那個方向，所以梯度下降法，其實就是朝著斜率最大的方向走。

![Pic](https://github.com/hung890202/ai110b/blob/master/note/%E5%9C%96%E7%89%87/梯度下降法.jpg)

當要用程式計算梯度時，我們可以從梯度的定義中看到他的基本元素，也就是「偏微分」。

因此我們先用程式寫出計算偏微分的程式:

```
def df(f, p, k, step=0.01):
    p1 = p.copy()
    p1[k] = p[k]+step
    return (f(p1) - f(p)) / step
 
利用以下指令就能夠計算出 f(x,y) 在 (1,1) 這點的偏導數
p = [1.0, 1.0]
print('nn.df(f, p, 0) = ', nn.df(f, p, 0))	
```

能夠算偏微分之後，我們就能夠利用npGradient.py算出梯度了 在學會計算梯度後，就可以開始使用梯度下降法了!

* 下圖為xx + yy的圖形:

![Pic](https://github.com/hung890202/ai110b/blob/master/note/%E5%9C%96%E7%89%87/xx+yy.jpg)

在執行程式後可以看到他最後找到了(0,0)這個點

```
1:p=[1.0, 3.0] f(p)=10.000 gp=[2.009999999999934, 6.009999999999849] glen=6.33721
2:p=[0.9799 2.9399] f(p)=9.603 gp=[1.9698 5.8898] glen=6.21046
3:p=[0.960202 2.881002] f(p)=9.222 gp=[1.930404 5.772004] glen=6.08625
4:p=[0.94089796 2.82328196] f(p)=8.856 gp=[1.89179592 5.65656392] glen=5.96453
5:p=[0.92198    2.76671632] f(p)=8.505 gp=[1.85396    5.54343264] glen=5.84524
(中間省略)
658:p=[-0.00499827 -0.00499483] f(p)=0.000 gp=[3.45722613e-06 1.03372781e-05] glen=0.00001
659:p=[-0.00499831 -0.00499493] f(p)=0.000 gp=[3.38808161e-06 1.01305326e-05] glen=0.00001
660:p=[-0.00499834 -0.00499504] f(p)=0.000 gp=[3.32031998e-06 9.92792193e-06] glen=0.00001
661:p=[-0.00499837 -0.00499514] f(p)=0.000 gp=[3.25391358e-06 9.72936349e-06] glen=0.00001
662:p=[-0.00499841 -0.00499523] f(p)=0.000 gp=[3.18883531e-06 9.53477622e-06] glen=0.00001
663:p=[-0.00499844 -0.00499533] f(p)=0.000 gp=[3.1250586e-06 9.3440807e-06] glen=0.00001
```